{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at classifying digits with Neural Networks. TODO: Explain MNIST a bit here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numClasses = 10\n",
    "inputSize = 784  \n",
    "numHiddenUnits = 256\n",
    "trainingIterations = 10000\n",
    "batchSize = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = [None, inputSize])\n",
    "y = tf.placeholder(tf.float32, shape = [None, numClasses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, onto the weight matrices and bias terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#W1 = tf.Variable(tf.constant(0.01, shape = [inputSize, numHiddenUnits]))\n",
    "#B1 = tf.Variable(tf.constant(0.01), [numHiddenUnits])\n",
    "#W2 = tf.Variable(tf.constant(0.01, shape = [numHiddenUnits, numClasses]))\n",
    "#B2 = tf.Variable(tf.constant(0.01), [numClasses])\n",
    "\n",
    "# Initialization is so crazy important with neural networks. The above didn't\n",
    "# converge, cuz the values got way too high, so I guess pulling from a normal \n",
    "# distribution instead of constants for initialization is the way to go. \n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([inputSize, numHiddenUnits], stddev=0.1))\n",
    "B1 = tf.Variable(tf.constant(0.01), [numHiddenUnits])\n",
    "W2 = tf.Variable(tf.random_normal([numHiddenUnits, numClasses], stddev=0.1))\n",
    "B2 = tf.Variable(tf.constant(0.01), [numClasses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hiddenLayerOutput = tf.matmul(X, W1) + B1\n",
    "hiddenLayerOutput = tf.nn.relu(hiddenLayerOutput)\n",
    "finalOutput = tf.matmul(hiddenLayerOutput, W2) + B2\n",
    "finalOutput = tf.nn.relu(finalOutput)\n",
    "pred = tf.nn.softmax(finalOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred) + (1-y)*tf.log(1-pred), reduction_indices=1))\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate = .001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The following are the statements that help with calculating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.03125\n",
      "step 1000, training accuracy 0.359375\n",
      "step 2000, training accuracy 0.40625\n",
      "step 3000, training accuracy 0.390625\n",
      "step 4000, training accuracy 0.515625\n",
      "step 5000, training accuracy 0.59375\n",
      "step 6000, training accuracy 0.515625\n",
      "step 7000, training accuracy 0.578125\n",
      "step 8000, training accuracy 0.5625\n",
      "step 9000, training accuracy 0.71875\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(trainingIterations):\n",
    "    batch = mnist.train.next_batch(batchSize)\n",
    "    batchInput = batch[0]\n",
    "    batchLabels = batch[1]\n",
    "    _, trainingLoss = sess.run([opt, loss], feed_dict={X: batchInput, y: batchLabels})\n",
    "    if i%1000 == 0:\n",
    "        trainAccuracy = accuracy.eval(session=sess, feed_dict={X: batchInput, y: batchLabels})\n",
    "        #train_loss = loss.eval(session=sess, feed_dict={X: batchInput, y: batchLabels})\n",
    "        print (\"step %d, training accuracy %g\"%(i, trainAccuracy))\n",
    "        #print (\"step %d, training loss %g\"%(i, train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.625\n"
     ]
    }
   ],
   "source": [
    "# They seem a bit low\n",
    "batch = mnist.test.next_batch(batchSize)\n",
    "testLoss = sess.run(accuracy, feed_dict={X: batch[0], y: batch[1]})\n",
    "print (\"test accuracy %g\"%(testLoss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Layer Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.0625\n",
      "step 1000, training accuracy 0.75\n",
      "step 2000, training accuracy 0.765625\n",
      "step 3000, training accuracy 0.90625\n",
      "step 4000, training accuracy 0.90625\n",
      "step 5000, training accuracy 0.984375\n",
      "step 6000, training accuracy 0.921875\n",
      "step 7000, training accuracy 0.90625\n",
      "step 8000, training accuracy 0.953125\n",
      "step 9000, training accuracy 0.9375\n",
      "test accuracy 0.9375\n"
     ]
    }
   ],
   "source": [
    "numHiddenUnitsLayer2 = 512\n",
    "tf.reset_default_graph() \n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, inputSize])\n",
    "y = tf.placeholder(tf.float32, shape = [None, numClasses])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([inputSize, numHiddenUnits], stddev=0.1))\n",
    "B1 = tf.Variable(tf.constant(0.01), [numHiddenUnits])\n",
    "W2 = tf.Variable(tf.random_normal([numHiddenUnits, numHiddenUnitsLayer2], stddev=0.1))\n",
    "B2 = tf.Variable(tf.constant(0.01), [numHiddenUnitsLayer2])\n",
    "W3 = tf.Variable(tf.random_normal([numHiddenUnitsLayer2, numClasses], stddev=0.1))\n",
    "B3 = tf.Variable(tf.constant(0.01), [numClasses])\n",
    "\n",
    "hiddenLayerOutput = tf.matmul(X, W1) + B1\n",
    "hiddenLayerOutput = tf.nn.relu(hiddenLayerOutput)\n",
    "hiddenLayer2Output = tf.matmul(hiddenLayerOutput, W2) + B2\n",
    "hiddenLayer2Output = tf.nn.relu(hiddenLayer2Output)\n",
    "finalOutput = tf.matmul(hiddenLayer2Output, W3) + B3\n",
    "finalOutput = tf.nn.relu(finalOutput)\n",
    "pred = tf.nn.softmax(finalOutput)\n",
    "\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred) + (1-y)*tf.log(1-pred), reduction_indices=1))\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate = .001).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(trainingIterations):\n",
    "    batch = mnist.train.next_batch(batchSize)\n",
    "    batchInput = batch[0]\n",
    "    batchLabels = batch[1]\n",
    "    _, trainingLoss = sess.run([opt, loss], feed_dict={X: batchInput, y: batchLabels})\n",
    "    if i%1000 == 0:\n",
    "        train_accuracy = accuracy.eval(session=sess, feed_dict={X: batchInput, y: batchLabels})\n",
    "        print (\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "\n",
    "batch = mnist.test.next_batch(batchSize)\n",
    "testLoss = sess.run(accuracy, feed_dict={X: batch[0], y: batch[1]})\n",
    "print (\"test accuracy %g\"%(testLoss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Definitely something with changing the number of layers and seeing the corresponding changes in accuracy\n",
    "* Maybe trying this approach with the Boston dataset since you've gone over that before?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stanford's Neural Network [Tutorial](http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/)\n",
    "* Michael Neilson's [Online Book](http://neuralnetworksanddeeplearning.com/)\n",
    "* Andrej Karpathy's [Blog Post](http://karpathy.github.io/neuralnets/ )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
